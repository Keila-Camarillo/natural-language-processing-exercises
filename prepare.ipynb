{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2537d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import acquire as a\n",
    "import prepare as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "30d7b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Paul Erdős and George Pólya were influential Hungarian mathematicians who contributed \\\n",
    "a lot to the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), \\\n",
    "but is often incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcbe23e",
   "metadata": {},
   "source": [
    "1. Define a function named basic_clean. It should take in a string and apply some basic text cleaning to it:\n",
    "\n",
    "Lowercase everything\n",
    "\n",
    "Normalize unicode characters\n",
    "\n",
    "Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6636dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase everything\n",
    "example = example.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a79ae03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field. erdos's name contains the hungarian letter 'o' ('o' with double acute accent), but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize unicode characters\n",
    "example = unicodedata.normalize('NFKD', example)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8', 'ignore')\n",
    "\n",
    "example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0d4e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\n"
     ]
    }
   ],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "example = re.sub(r\"[^a-z0-9'\\s]\", '', example)\n",
    "print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdf8f7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    # lowercase everything\n",
    "    string = string.lower()\n",
    "    \n",
    "    # Normalize unicode characters\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "    \n",
    "    # remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "    return re.sub(r\"[^a-z0-9'\\s]\", '', string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81870c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check import function\n",
    "example = p.basic_clean(example)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bc9b6",
   "metadata": {},
   "source": [
    "2. Define a function named tokenize. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e97de52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "496f09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "    return tokenizer.tokenize(string, return_str=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb45a528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field erdos ' s name contains the hungarian letter ' o ' ' o ' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c178f34",
   "metadata": {},
   "source": [
    "3. Define a function named stem. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d295f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    #create porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    stemmed = ' '.join(stems)\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da7dcfeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya were influenti hungarian mathematician who contribut a lot to the field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is often incorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48111531",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create porter stemmer\n",
    "ps = nltk.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ed4b62b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('call', 'call', 'call', 'call')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test stemmer\n",
    "ps.stem('calling'), ps.stem('calls'), ps.stem('called'), ps.stem('call')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbc7a40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mous', 'mice')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('mouse'), ps.stem('mice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2bd23b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematicians who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necess\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use stemmer \n",
    "ps.stem(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6dc6ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdo',\n",
       " 'and',\n",
       " 'georg',\n",
       " 'polya',\n",
       " 'were',\n",
       " 'influenti',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use stemmer - apply stem to each word in our string\n",
    "stems = [ps.stem(word) for word in example.split()]\n",
    "stems[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16cfcbb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya were influenti hungarian mathematician who contribut a lot to the field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is often incorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "article_stemmed = ' '.join(stems)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cec62c",
   "metadata": {},
   "source": [
    "4. Define a function named lemmatize. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "119af2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    lemma = ' '.join(lemmas)\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d02785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = lemmatize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bc01f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the first time\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "690fe28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordNetLemmatizer>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the lemmatizer\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "wnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35432fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mouse', 'mouse')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test lemmatizer\n",
    "wnl.lemmatize('mouse'), wnl.lemmatize('mice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fe4ab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'and',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'were',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use lemmatize - apply stem to each word in our string\n",
    "# wnl.lemmatize(article)\n",
    "lemmas = [wnl.lemmatize(word) for word in example.split()]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a4b6e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya were influential hungarian mathematician who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written a erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "article_lemma = ' '.join(lemmas)\n",
    "article_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac845bd7",
   "metadata": {},
   "source": [
    "4. Define a function named remove_stopwords. It should accept some text and return the text after removing all the stopwords.\n",
    "\n",
    "This function should define two optional parameters, extra_words and exclude_words. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "df0bf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stopwords list\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5fec34d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kcamarillo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only need to do once\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2106f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text, extra_words=None, exclude_words=None):\n",
    "    # Get the default English stopwords list from NLTK\n",
    "    stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "    # Add extra words to the stopwords list\n",
    "    if extra_words:\n",
    "        stopwords_list.update(extra_words)\n",
    "\n",
    "    # Remove exclude words from the stopwords list\n",
    "    if exclude_words:\n",
    "        stopwords_list.difference_update(set(exclude_words))\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords from the list of words\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords_list]\n",
    "\n",
    "    # Join the filtered words back into a string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "188e8e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematician lot field erdos 's name contains hungarian letter double acute accent often incorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(example, extra_words=[\"'\", \"contributed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2fa2737a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save stopwords\n",
    "stopwords_ls = stopwords.words('english')\n",
    "stopwords_ls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "80e07e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ls.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a71a5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_ls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32f511e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'about', 'after']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set a list to remove some stopwords\n",
    "extra = ['all','about','after']\n",
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a2c445c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'above',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords_ls) - set(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26e4fa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'and',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'were',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split words in lemmatized article\n",
    "words = article_lemma.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6baa1701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'contributed',\n",
       " 'lot',\n",
       " 'field',\n",
       " \"erdos's\",\n",
       " 'name',\n",
       " 'contains',\n",
       " 'hungarian',\n",
       " 'letter',\n",
       " \"'o'\",\n",
       " \"'o'\",\n",
       " 'double',\n",
       " 'acute',\n",
       " 'accent',\n",
       " 'often',\n",
       " 'incorrectly',\n",
       " 'written',\n",
       " 'erdos',\n",
       " 'erdos',\n",
       " 'either',\n",
       " 'mistake',\n",
       " 'typographical',\n",
       " 'necessity']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords from list of words\n",
    "[word for word in words if word not in stopwords_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7b5dbed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ls.append(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1f136906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'contributed',\n",
       " 'lot',\n",
       " 'field']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = [word for word in words if word not in stopwords_ls]\n",
    "filtered[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7407add1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show how many words we removed\n",
    "len(words) - len(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "057da3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematician contributed lot field erdos's name contains hungarian letter 'o' 'o' double acute accent often incorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join words back together\n",
    "parsed_article =' '.join(filtered) \n",
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffe16f",
   "metadata": {},
   "source": [
    "6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe news_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d552e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = a.get_news_articles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9a2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn into df\n",
    "news_df = pd.DataFrame(news_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ab5c58",
   "metadata": {},
   "source": [
    "7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b43420",
   "metadata": {},
   "outputs": [],
   "source": [
    "codeup_df = pd.DataFrame(a.get_blog_articles())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32a7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "codeup_df = codeup_df.rename(columns={\"content\": \"original\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e930a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bf/lhpb5n_j1xj4msxy6nrm9h7h0000gn/T/ipykernel_10692/1231594132.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/natural-language-processing-exercises/prepare.py\u001b[0m in \u001b[0;36mbasic_clean\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbasic_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# lowercase everything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Normalize unicode characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "import prepare as p\n",
    "p.basic_clean([news_df[\"content\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c24ad",
   "metadata": {},
   "source": [
    "8. For each dataframe, produce the following columns:\n",
    "\n",
    "title to hold the title\n",
    "original to hold the original article/post content\n",
    "clean to hold the normalized and tokenized original with the stopwords removed.\n",
    "stemmed to hold the stemmed version of the cleaned data.\n",
    "lemmatized to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8d1ce9",
   "metadata": {},
   "source": [
    "9. Ask yourself:\n",
    "\n",
    "If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd3a9b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
